{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ad2789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "import torch.nn.functional as F\n",
    "from Bio import AlignIO\n",
    "import pandas as pd\n",
    "import torch\n",
    "import esm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from Bio.PDB import PDBParser, PPBuilder\n",
    "import warnings\n",
    "from Bio.PDB.PDBExceptions import PDBConstructionWarning\n",
    "from Bio.SeqUtils import IUPACData\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import Bio.PDB\n",
    "import random\n",
    "import glob\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0be7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/Users/williamharrigan/Desktop/Github/contact_site_classifier/attention_classifier/data_files/'\n",
    "desktop = '/Users/williamharrigan/Desktop/'\n",
    "fasta_file = base_dir + 'rcsb_pdb_3KYN.fasta'\n",
    "pdb_filename = base_dir + '3kyn.pdb'\n",
    "structure_dir = base_dir +'structure_files/'\n",
    "fasta_dir = base_dir +'fasta_files/'\n",
    "casp = '/Users/williamharrigan/Desktop/UH/Year_2/Research/contact_site_classifier/casp7/' + 'training_95'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9df12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code is not important right now, will be when adding multiple sequences\n",
    "\n",
    "# seqs = SeqIO.to_dict(SeqIO.parse(full_len_sequences, \"fasta\"))\n",
    "# for k,v in seqs.items():\n",
    "#     seqs[k] = str(v.seq)\n",
    "    \n",
    "# print(list(seqs.keys())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7f3b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_aa(three_letter_code):\n",
    "    return IUPACData.protein_letters_3to1.get(three_letter_code.capitalize())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5322af",
   "metadata": {},
   "source": [
    "## To Do:\n",
    "\n",
    "\n",
    "1. Get sequence IDs from CASP training file\n",
    "2. Extract sequence from sequence ID from PDB database\n",
    "3. Extract .pdb structural file from PDB for same ID\n",
    "4. Double check that seqeunce files that are extracted are the same as the structural files\n",
    "5. Get embeddings from extracted sequences (try 100 sequences first)\n",
    "6. Turn contact site code into functions that can be run iteratively\n",
    "7. Integrate this code with the classifier\n",
    "    1. Make sure that when training model we balance positive and negative cases. (Start with 50/50 balance)\n",
    "    2. Do so by making contact and non-contact dictionaries to pull training data from\n",
    "8. Start working on implementing statstics like AUROC, loss functions and other hyperparameter optimization tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10dff23",
   "metadata": {},
   "source": [
    "## Extract Sequence IDs and Structure (.pdb) files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b548d700",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_casp7_file(file_path):\n",
    "    data_dict = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith('[ID]'):\n",
    "                sequence_id = next(file).strip()\n",
    "            elif line.startswith('[PRIMARY]'):\n",
    "                sequence = next(file).strip()\n",
    "#                 print(sequence_id, sequence)\n",
    "                data_dict[sequence_id] = sequence\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f732a5ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def download_pdb(pdb_id, structure_dir, downloadurl=\"http://files.rcsb.org/download/\"):\n",
    "    \"\"\"\n",
    "    Downloads a PDB file from the Internet and saves it in a data directory.\n",
    "    :param pdbcode: The standard PDB ID e.g. '3ICB' or '3icb'\n",
    "    :param datadir: The directory where the downloaded file will be saved\n",
    "    :param downloadurl: The base PDB download URL, cf.\n",
    "        `https://www.rcsb.org/pages/download/http#structures` for details\n",
    "        Note that the unencrypted HTTP protocol is used by default\n",
    "        to avoid spurious OpenSSL errors...\n",
    "    :return: the full path to the downloaded PDB file or None if something went wrong\n",
    "    \"\"\"\n",
    "    pdbfn = pdb_id + \".pdb\"\n",
    "    url = downloadurl + pdbfn\n",
    "    outfnm = os.path.join(structure_dir, pdbfn)\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, outfnm)\n",
    "        return pdbfn\n",
    "    except Exception as err:\n",
    "        # all sorts of things could have gone wrong...\n",
    "        print(str(err), file=sys.stderr)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c002d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_fasta(pdb_id, fasta_dir, downloadurl=\"https://www.rcsb.org/fasta/entry/\"):\n",
    "    pdbfn = pdb_id\n",
    "    url = downloadurl + pdbfn\n",
    "    outfnm = os.path.join(fasta_dir, F'{pdbfn}.fasta')\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, outfnm)\n",
    "        return pdbfn\n",
    "    except Exception as err:\n",
    "        # all sorts of things could have gone wrong...\n",
    "        print(str(err), file=sys.stderr)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3318992",
   "metadata": {},
   "outputs": [],
   "source": [
    "### HERE\n",
    "# Turned off warnings for discontinuous data structures\n",
    "\n",
    "warnings.simplefilter('ignore', PDBConstructionWarning)\n",
    "\n",
    "# Load the structure from locally saved file\n",
    "parser = PDBParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaaf486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sequences(pdb_id):\n",
    "    # Parse pdb file and save as structure. The pdb file is where we are getting CA coordinates from.\n",
    "    structure = parser.get_structure(pdb_id, f\"{structure_dir+pdb_id}.pdb\")\n",
    "\n",
    "    # Extract desired protein structure from PDB structure (typically only 1 structure to choose from)\n",
    "    protein_structure = structure[0]\n",
    "\n",
    "    residue_position = 0\n",
    "    mismatches = 0\n",
    "#     print(pdb_id)\n",
    "    if 'A' in protein_structure:\n",
    "        for residue in protein_structure['A']:\n",
    "            if 'CA' in residue:\n",
    "                if residue_position < len(protein_data[pdb_id]):\n",
    "                    if simple_aa(residue.resname) != protein_data[pdb_id][residue_position]:\n",
    "#                         print(residue.id[1], simple_aa(residue.resname), protein_data[pdb_id][residue_position])\n",
    "                        mismatches+=1\n",
    "            residue_position+=1\n",
    "        if mismatches == 0:\n",
    "            same_sequence_ids.append(pdb_id)\n",
    "#         print(pdb_id)\n",
    "    return pdb_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bc5425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Prepare sequence input from a fasta file and already loaded .pdb file\n",
    "# Load sequence file\n",
    "\n",
    "# seq_chains = []\n",
    "\n",
    "# for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "#     seq_chains.append(str(record.seq))\n",
    "\n",
    "# pdb_id = record.id.split('|')[0].split('_')[0]\n",
    "# protein_sequence = ''.join(seq_chains)\n",
    "# Replace 'training_30.txt' with the path to your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8433b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sequences from CASP7 file\n",
    "prot_data_dict = parse_casp7_file(casp)\n",
    "\n",
    "protein_id_counts = Counter(protein_id.split('_')[0] for protein_id in prot_data_dict)\n",
    "\n",
    "# Now, create the test list with only those protein_ids that occur exactly once\n",
    "single_occurence_ids = [protein_id for protein_id, count in protein_id_counts.items() if count == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70184fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Load sequence file\n",
    "\n",
    "# protein_data = {}\n",
    "\n",
    "# for pdb_id in single_occurence_ids:\n",
    "#     check = download_fasta(pdb_id, fasta_dir)\n",
    "#     if check == None:\n",
    "#         continue\n",
    "#     else:\n",
    "# #         download_pdb(pdb_id, structure_dir)\n",
    "#         fasta_file = fasta_dir + pdb_id + '.fasta'\n",
    "#         for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "#             print(record.id.split('|')[0].split('_')[0])\n",
    "#             print(len(str(record.seq)))\n",
    "#             protein_data[record.id.split('|')[0].split('_')[0]] = str(record.seq)\n",
    "#             break\n",
    "\n",
    "# # pdb_id = record.id.split('|')[0].split('_')[0]\n",
    "# # protein_sequence = ''.join(seq_chains)\n",
    "# # Replace 'training_30.txt' with the path to your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daa0270",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take pdb_ids that occur only once in CASP7 dataset and generate fasta files \n",
    "# of sequences from pdb\n",
    "# Only sequences in first chain are taken to keep things simple down the line\n",
    "\n",
    "\n",
    "# protein_data = {}\n",
    "\n",
    "# for pdb_id in single_occurence_ids:\n",
    "#     fasta_file = fasta_dir + pdb_id + '.fasta'\n",
    "#     for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "#         print(record.id.split('|')[0].split('_')[0])\n",
    "#         print(len(str(record.seq)))\n",
    "#         protein_data[record.id.split('|')[0].split('_')[0]] = str(record.seq)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1767c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IF FASTA FILES ALREADY GENERATED\n",
    "## Load pdb_ids and corresponding sequence from already generated fasta files in fasta_dir\n",
    "## Only sequences in first chain are taken to keep things simple down the line\n",
    "## SHOULD BE 2681 KEYS\n",
    "\n",
    "protein_data = {}\n",
    "\n",
    "for i in glob.glob(f'{fasta_dir}/*'):\n",
    "#     pdb_id = i.split('/')[9].split('.')[0]\n",
    "    fasta_file = i\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        print(record.id.split('|')[0].split('_')[0])\n",
    "        print(len(str(record.seq)))\n",
    "        protein_data[record.id.split('|')[0].split('_')[0]] = str(record.seq)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65b63ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Prepare sequence input to ESM/contact analysis from CASP7 dataset\n",
    "# # Load CASP file containing PDB IDs and Sequences\n",
    "\n",
    "# protein_data = parse_casp7_file(casp)\n",
    "# same_sequence_ids = []\n",
    "\n",
    "# for pdb_id, protein_sequence in list(protein_data.items()):\n",
    "#     print('Sequence ID: ', pdb_id)\n",
    "# #     print('Sequence: ', protein_sequence, len(protein_sequence), '\\n')\n",
    "    \n",
    "#     # Download pdb structure files from PDB database using CASP7 protein ids\n",
    "#     print('PDB Structural File Output: ', download_pdb(pdb_id, structure_dir), '\\n')\n",
    "#     check_sequences(pdb_id)\n",
    "# #     Index protein sequence as sequence 0 (next sequence would be indexed as 1)\n",
    "#     esm_input_data = [(0, protein_sequence)]\n",
    "#     print('Data: ', esm_input_data, '\\n')\n",
    "\n",
    "#     # Prepare variables to input sequence into ESM-2 model \n",
    "#     batch_converter = alphabet.get_batch_converter()\n",
    "#     batch_labels, batch_strs, batch_tokens = batch_converter(esm_input_data)\n",
    "#     batch_tokens = batch_tokens.cuda() if torch.cuda.is_available() else batch_tokens\n",
    "    \n",
    "#     print('batch_tokens: ', '\\n\\n', batch_tokens, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5657ff9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## IF PDB FILES ARE ALREADY LOADED\n",
    "# Should have 1215 perfect match sequences (same_sequence_ids)\n",
    "# protein_data = parse_casp7_file(casp)\n",
    "\n",
    "same_sequence_ids = []\n",
    "iterations = 0\n",
    "for pdb_id, protein_sequence in list(protein_data.items()):\n",
    "    print(\"Iterations: \", iterations)\n",
    "    print('Sequence ID: ', pdb_id)\n",
    "    print('No mismatches in pdb sequence and pulled sequence: ', check_sequences(pdb_id))\n",
    "    iterations+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbb1c65",
   "metadata": {},
   "source": [
    "## Calculating Expected Contact Sites from PDB structure file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a0c50e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parser = PDBParser()\n",
    "# structure = parser.get_structure(pdb_id, f\"{structure_dir+pdb_id}.pdb\")\n",
    "\n",
    "# # Extract desired protein structure from PDB structure (typically only 1 structure to choose from)\n",
    "# protein_structure = structure[0]\n",
    "\n",
    "\n",
    "# for chain in protein_structure:\n",
    "#     print(\"Chain:\", chain.id)  # Reset position for each chain\n",
    "\n",
    "#     for residue in chain:\n",
    "# #         print(residue.id)\n",
    "#         if residue_position >= len(protein_data[pdb_id]):\n",
    "#             print(\"Sequence length exceeded at chain\", chain.id)\n",
    "#             break  # Break the loop if the position exceeds the sequence length\n",
    "#         try:\n",
    "#             if simple_aa(residue.resname) != protein_data[pdb_id][residue_position]:\n",
    "#                 print(\"Mismatch at position:\", residue.id[1])\n",
    "#                 break\n",
    "#             else:\n",
    "#                 if simple_aa(residue.resname) == None:\n",
    "#                     continue\n",
    "#                 else:\n",
    "#                     print(residue.id[1], simple_aa(residue.resname), protein_data[pdb_id][residue_position], residue['CA'])\n",
    "#                 residue_position += 1\n",
    "            \n",
    "#         except KeyError:\n",
    "#             # Specific exception handling\n",
    "#             continue\n",
    "#     print(residue_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21932ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_contact_sites(pdb_id):\n",
    "    structure = parser.get_structure(pdb_id, f\"{structure_dir}/{pdb_id}.pdb\")  # Ensure correct path joining\n",
    "    protein_structure = structure[0]\n",
    "    chain = protein_structure['A']\n",
    "\n",
    "    # Initialize count variable\n",
    "    count = 0\n",
    "\n",
    "    for i, residue1 in enumerate(chain):\n",
    "        for j, residue2 in enumerate(chain):\n",
    "            if i <= j:\n",
    "                continue # Avoids redundant comparisons and self-comparison\n",
    "            if residue1.id[1] > len(protein_data[pdb_id]) or residue2.id[1] > len(protein_data[pdb_id]):\n",
    "                continue\n",
    "            try:\n",
    "                distance = abs(residue1['CA'] - residue2['CA'])\n",
    "            except KeyError:\n",
    "                continue\n",
    "            if distance < 5:\n",
    "                if abs(residue1.id[1] - residue2.id[1]) > 2:\n",
    "    #                 print(residue1.id[1], residue1.resname, residue2.id[1], residue2.resname, distance)\n",
    "                    in_contact_sites[pdb_id].append({\n",
    "                        'res_1': residue1.id[1], \n",
    "                        'res_2': residue2.id[1], \n",
    "                        'sig_1': simple_aa(residue1.resname), \n",
    "                        'sig_2': simple_aa(residue2.resname), \n",
    "                        'dist': distance,\n",
    "                        'in_contact': True\n",
    "                    })\n",
    "                    count += 1\n",
    "            else:\n",
    "                if abs(residue1.id[1] - residue2.id[1]) > 2:\n",
    "                    non_contact_sites[pdb_id].append({\n",
    "                        'res_1': residue1.id[1], \n",
    "                        'res_2': residue2.id[1], \n",
    "                        'sig_1': simple_aa(residue1.resname), \n",
    "                        'sig_2': simple_aa(residue2.resname), \n",
    "                        'dist': distance,\n",
    "                        'in_contact': False\n",
    "                    })\n",
    "\n",
    "    if non_contact_sites[pdb_id]:\n",
    "        subset_non_contact_sites[pdb_id] = random.sample(non_contact_sites[pdb_id], min(len(non_contact_sites[pdb_id]), len(in_contact_sites[pdb_id])))\n",
    "\n",
    "    # Optionally print or process the results\n",
    "    return f\"Total contacts found {pdb_id}: {count}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c964da8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "in_contact_sites = defaultdict(list)\n",
    "non_contact_sites = defaultdict(list)\n",
    "subset_non_contact_sites = defaultdict(list)\n",
    "\n",
    "iterations = 0\n",
    "\n",
    "for pdb_id in same_sequence_ids:\n",
    "    print(calc_contact_sites(pdb_id))\n",
    "    print(len(in_contact_sites[pdb_id]), len(subset_non_contact_sites[pdb_id]))\n",
    "    print(\"Iterations: \", iterations)\n",
    "    iterations+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf2d0cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(in_contact_sites)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a37b1e3",
   "metadata": {},
   "source": [
    "## Initialize Contact Dictionaries for Training Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3d6864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# # Assuming protein_structure and protein_sequence are defined\n",
    "\n",
    "# contacts = defaultdict(dict)\n",
    "\n",
    "# # Mapping of chain IDs to starting index\n",
    "# chain_to_index = {'A': -1, 'B': 275, 'P': 375}\n",
    "\n",
    "# count = 0\n",
    "\n",
    "\n",
    "# ### Contacts from the SAME chain are only being considered\n",
    "\n",
    "# for chain in protein_structure:\n",
    "# #     print(chain.id)\n",
    "#     index = chain_to_index.get(chain.id, 0)  # Default to 0 if chain ID not found\n",
    "\n",
    "#     for residue1 in chain:\n",
    "#         for residue2 in chain:\n",
    "#             if residue1 != residue2:\n",
    "#                 try:\n",
    "#                     # Calculate Alpha-Carbon Distance\n",
    "#                     distance = abs(residue1['CA'] - residue2['CA'])\n",
    "#                 except KeyError:\n",
    "#                     continue\n",
    "\n",
    "#                 # Calculating Distance of Amino Acids in Sequence\n",
    "#                 diff = abs(residue1.id[1] - residue2.id[1])\n",
    "#                 if diff > 2:\n",
    "#                     res1_index = residue1.id[1] + index\n",
    "#                     res2_index = residue2.id[1] + index\n",
    "\n",
    "#                     # Ensure indices are within the bounds of protein_sequence\n",
    "#                     if 0 <= res1_index < len(protein_sequence) and 0 <= res2_index < len(protein_sequence):\n",
    "#                         contact = distance < 5\n",
    "#                         contacts[(res1_index, res2_index)] = {\n",
    "#                             'aa1': protein_sequence[res1_index],\n",
    "#                             'aa2': protein_sequence[res2_index], \n",
    "#                             'dist': distance,\n",
    "#                             'contact': contact\n",
    "#                         }\n",
    "#                         if contact:\n",
    "#                             count += 1\n",
    "# #                         print(res1_index, residue1.resname, res2_index, residue2.resname, distance, protein_sequence[res1_index], protein_sequence[res2_index], contact)\n",
    "\n",
    "# print(f\"Total contacts: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95546996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize contact_data as a defaultdict of lists\n",
    "contact_data = defaultdict(list)\n",
    "\n",
    "# Add data from in_contact_sites\n",
    "for pdb_id, contacts in in_contact_sites.items():\n",
    "    for contact in contacts:\n",
    "        contact_data[pdb_id].append({\n",
    "            'res_1': contact['res_1'],\n",
    "            'res_2': contact['res_2'],\n",
    "            'sig_1': contact['sig_1'],\n",
    "            'sig_2': contact['sig_2'],\n",
    "            'dist': contact['dist'],\n",
    "            'in_contact': contact['in_contact']\n",
    "        })\n",
    "\n",
    "# Add data from subset_non_contact_sites\n",
    "for pdb_id, non_contacts in subset_non_contact_sites.items():\n",
    "    for non_contact in non_contacts:\n",
    "        contact_data[pdb_id].append({\n",
    "            'res_1': non_contact['res_1'],\n",
    "            'res_2': non_contact['res_2'],\n",
    "            'sig_1': non_contact['sig_1'],\n",
    "            'sig_2': non_contact['sig_2'],\n",
    "            'dist': non_contact['dist'],\n",
    "            'in_contact': non_contact['in_contact']\n",
    "        })\n",
    "\n",
    "# contact_data is now a defaultdict containing all the data from both dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfa44b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Assuming contact_data is defined and is a list of dictionaries\n",
    "# total_data = same_sequence_ids\n",
    "\n",
    "# # First split: Splitting into 80% for training and 20% for the remaining\n",
    "# train_data, remaining_data = train_test_split(total_data, test_size=0.20, random_state=42)\n",
    "\n",
    "# # Second split: Splitting the remaining 20% into 15% test and 5% validation of the original data\n",
    "# # This corresponds to 75% (test) and 25% (validation) of the remaining 20% data\n",
    "# test_data, validation_data = train_test_split(remaining_data, test_size=0.25, random_state=42)\n",
    "\n",
    "# # Print the sizes to verify\n",
    "# print(f\"Training set size: {len(train_data)}\")\n",
    "# print(f\"Test set size: {len(test_data)}\")\n",
    "# print(f\"Validation set size: {len(validation_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa095c9",
   "metadata": {},
   "source": [
    "## Generate ESM-2 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf774bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the ESM Model\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eaf392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to use cuda GPU\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beba56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index protein sequence as sequence 0 (next sequence would be indexed as 1)\n",
    "\n",
    "def generate_embeddings(pdb_id):\n",
    "    protein_sequence = protein_data[pdb_id]\n",
    "    esm_input_data = [(0, protein_sequence)]\n",
    "    # print('Data: ', esm_input_data, '\\n')\n",
    "\n",
    "    # Prepare variables to input sequence into ESM-2 model \n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(esm_input_data)\n",
    "    batch_tokens = batch_tokens.cuda() if torch.cuda.is_available() else batch_tokens\n",
    "\n",
    "    # print('batch_tokens: ', '\\n\\n', batch_tokens, '\\n')\n",
    "\n",
    "    # 4. Input prepared sequence information into model and output as results (contact predictions are included in embedding)\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "\n",
    "    return results['attentions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265dbac3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# attention_data = {}\n",
    "# iterations = 0\n",
    "# for pdb_id in train_data[:300]:\n",
    "#     attention_data[pdb_id] = generate_embeddings(pdb_id)\n",
    "#     print(\"Iterations: \", iterations)\n",
    "#     iterations+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96d3152",
   "metadata": {},
   "source": [
    "## Implementing Random Forest Classifier (Currently for only 1 sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b94984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Modelling\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Tree Visualisation\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6575758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attentions from all heads and layers for given amino acid residues\n",
    "\n",
    "def get_x_y(attention_data, res_1, res_2):\n",
    "    vectors = []\n",
    "    for layer in range(0,33):\n",
    "        for head in range(0,20):\n",
    "            vectors.append(attention_data[0][layer][head][res_1][res_2])\n",
    "\n",
    "    return vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b4b34c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Extract attentions from all heads and layers for given amino acid residues\n",
    "\n",
    "# def get_x_y(pdb_id, res_1, res_2):\n",
    "#     vectors = []\n",
    "#     for layer in range(0,33):\n",
    "#         for head in range(0,20):\n",
    "#             vectors.append(attention_data[pdb_id][0][layer][head][res_1][res_2])\n",
    "\n",
    "#     return vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab7ed3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for pdb_id in train_data:    \n",
    "#     structure = parser.get_structure(pdb_id, f\"{structure_dir}/{pdb_id}.pdb\")  # Ensure correct path joining\n",
    "#     protein_structure = structure[0]\n",
    "#     chain = protein_structure['A']\n",
    "#     print(list(chain.get_residues())[0].id[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a72d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = []\n",
    "# y_train = []\n",
    "\n",
    "# for pdb_id in train_data:\n",
    "#     structure = parser.get_structure(pdb_id, f\"{structure_dir}/{pdb_id}.pdb\")  # Ensure correct path joining\n",
    "#     protein_structure = structure[0]\n",
    "#     chain = protein_structure['A']\n",
    "#     first_residue = list(chain.get_residues())[0].id[1]\n",
    "#     if first_residue == 1:\n",
    "#         for i in (contact_data[pdb_id]):\n",
    "#             if pdb_id in attention_data.keys():\n",
    "#                 X_train.append(get_x_y(pdb_id, i['res_1'], i['res_2']))\n",
    "#                 y_train.append(i['in_contact'])\n",
    "#         else:\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de2b551",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "iterations = 0\n",
    "\n",
    "for pdb_id in same_sequence_ids:\n",
    "    structure = parser.get_structure(pdb_id, f\"{structure_dir}/{pdb_id}.pdb\")  # Ensure correct path joining\n",
    "    protein_structure = structure[0]\n",
    "    chain = protein_structure['A']\n",
    "    first_residue = list(chain.get_residues())[0].id[1]\n",
    "    print('Iteration: ', iterations)\n",
    "    iterations+=1\n",
    "    if first_residue == 1:\n",
    "        attention_data = generate_embeddings(pdb_id)\n",
    "        for i in contact_data[pdb_id]:\n",
    "                X.append(get_x_y(attention_data, i['res_1'], i['res_2']))\n",
    "                y.append(i['in_contact'])\n",
    "        else:\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8]\n",
    "train_errors = []\n",
    "val_errors = []\n",
    "\n",
    "for i in splits:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=i, random_state=0)\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Record the training and validation errors\n",
    "    train_errors.append(1 - clf.score(X_train, y_train))\n",
    "    val_errors.append(1 - clf.score(X_test, y_test))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40282ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the error rates\n",
    "plt.figure(figsize=(10, 5))\n",
    "# plt.plot(splits, train_errors, label='Training Error')\n",
    "plt.plot(splits, val_errors, label='Error Rate', linestyle='--')\n",
    "plt.xlabel('Test Split Size')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.title('Random Forest Error Rate vs. Test Split Size')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11dedcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e1619",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_train = []\n",
    "# y_train = []\n",
    "# iterations = 0\n",
    "\n",
    "# for pdb_id in train_data:\n",
    "#     structure = parser.get_structure(pdb_id, f\"{structure_dir}/{pdb_id}.pdb\")  # Ensure correct path joining\n",
    "#     protein_structure = structure[0]\n",
    "#     chain = protein_structure['A']\n",
    "#     first_residue = list(chain.get_residues())[0].id[1]\n",
    "#     print('Iteration: ', iterations)\n",
    "#     iterations+=1\n",
    "#     if first_residue == 1:\n",
    "#         attention_data = generate_embeddings(pdb_id)\n",
    "#         for i in contact_data[pdb_id]:\n",
    "#                 X_train.append(get_x_y(attention_data, i['res_1'], i['res_2']))\n",
    "#                 y_train.append(i['in_contact'])\n",
    "#         else:\n",
    "#             continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97075792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit classifier\n",
    "\n",
    "# clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "# clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdee544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errors = []\n",
    "val_errors = []\n",
    "\n",
    "# Train Random Forest Classifier with different numbers of trees\n",
    "for n_trees in range(1, 101):  # Adjust the range as needed\n",
    "    clf = RandomForestClassifier(n_estimators=n_trees)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Record the training and validation errors\n",
    "    train_errors.append(1 - clf.score(X_train, y_train))\n",
    "    val_errors.append(1 - clf.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db83eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the error rates\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, 101), train_errors, label='Training Error')\n",
    "plt.plot(range(1, 101), val_errors, label='Validation Error', linestyle='--')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.title('Random Forest Error Rate vs. Number of Trees')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f934b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for pdb_id in test_data[:100]:\n",
    "#     attention_data[pdb_id] = generate_embeddings(pdb_id)\n",
    "#     print(\"Iterations: \", iterations)\n",
    "#     iterations+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f802d5e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_test = []\n",
    "# y_test = []\n",
    "# iterations = 0\n",
    "\n",
    "# for pdb_id in test_data:\n",
    "#     structure = parser.get_structure(pdb_id, f\"{structure_dir}/{pdb_id}.pdb\")  # Ensure correct path joining\n",
    "#     protein_structure = structure[0]\n",
    "#     chain = protein_structure['A']\n",
    "#     first_residue = list(chain.get_residues())[0].id[1]\n",
    "#     print('Iteration: ', iterations)\n",
    "#     iterations+=1\n",
    "#     if first_residue == 1:\n",
    "#         attention_data = generate_embeddings(pdb_id)\n",
    "#         for i in contact_data[pdb_id]:\n",
    "#                 X_test.append(get_x_y(attention_data, i['res_1'], i['res_2']))\n",
    "#                 y_test.append(i['in_contact'])\n",
    "#         else:\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700da199",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76c7740",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ae2e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Positive']).plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392d424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8391d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find X vectors that are supposed to be True\n",
    "\n",
    "# count = 0\n",
    "\n",
    "# for vec,cont in zip(X,y):\n",
    "#     count+=1\n",
    "#     if cont == True:\n",
    "#         print(count)\n",
    "#         print(vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
