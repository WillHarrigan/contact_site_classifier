{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4472542f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESM2(\n",
      "  (embed_tokens): Embedding(33, 1280, padding_idx=1)\n",
      "  (layers): ModuleList(\n",
      "    (0-32): 33 x TransformerLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (rot_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (contact_head): ContactPredictionHead(\n",
      "    (regression): Linear(in_features=660, out_features=1, bias=True)\n",
      "    (activation): Sigmoid()\n",
      "  )\n",
      "  (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): RobertaLMHead(\n",
      "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Set path to directory containing .py files\n",
    "# Import .py file to extract distance between amino acids, contacts and attentions\n",
    "# All necessary packages are imported within the .py file\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../py\")\n",
    "\n",
    "# ESM-2 model for protein embeddings is esm.pretrained.esm2_t33_650M_UR50D()\n",
    "from extract_contact_aa_distance_attentions import * \n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e55e177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings when importing PDB structure files\n",
    "warnings.simplefilter('ignore', PDBConstructionWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Set variable for parsing PDB structural files\n",
    "parser = PDBParser()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3705be40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
